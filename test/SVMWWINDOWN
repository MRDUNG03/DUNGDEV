import numpy as np
from scipy.fft import fft
from scipy.signal import windows
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler
import joblib

# --- TH√îNG S·ªê C·ªê ƒê·ªäNH ---
W = 512       # K√≠ch th∆∞·ªõc c·ª≠a s·ªï (m·∫´u/c·ª≠a s·ªï)
S = 512       # B∆∞·ªõc nh·∫£y (Stride)
FS = 512      # T·∫ßn s·ªë l·∫•y m·∫´u (Hz)

# --- B∆Ø·ªöC 0: D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO (Thay th·∫ø b·∫±ng d·ªØ li·ªáu th·ª±c t·∫ø 600k m·∫´u c·ªßa b·∫°n) ---
N_TOTAL_SAMPLES = 600000
t = np.arange(N_TOTAL_SAMPLES) / FS
# T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p Normal (v√≠ d·ª•: t√≠n hi·ªáu 5Hz)
raw_data_normal = 0.5 * np.sin(2 * np.pi * 5 * t) + np.random.normal(0, 0.1, N_TOTAL_SAMPLES)
print(f"T·ªïng d·ªØ li·ªáu Normal th√¥: {len(raw_data_normal)} m·∫´u.")
# -------------------------------------------------------------------------

# --- 2. H√ÄM TR√çCH XU·∫§T ƒê·∫∂C TR∆ØNG MI·ªÄN T·∫¶N S·ªê ---
def extract_frequency_features(window_data, W, FS):
    """Tr√≠ch xu·∫•t 5 ƒë·∫∑c tr∆∞ng mi·ªÅn t·∫ßn s·ªë (Ph·∫£i gi·ªëng h·ªát nhau khi Training v√† Testing)."""
    hanning_window = windows.hann(W)
    windowed_signal = window_data * hanning_window
    
    N_FFT = len(windowed_signal)
    Y = fft(windowed_signal)
    P2 = np.abs(Y / N_FFT)
    P1 = P2[:N_FFT//2]
    P1[1:-1] = 2 * P1[1:-1]
    f = FS * np.arange(0, N_FFT/2) / N_FFT

    # T√≠nh to√°n 5 ƒê·∫∑c tr∆∞ng (C√°c ƒë·∫∑c tr∆∞ng nƒÉng l∆∞·ª£ng ph·ªï bi·∫øn)
    total_energy = np.sum(P1**2) 
    center_frequency = np.sum(f * P1) / np.sum(P1)
    rms_frequency = np.sqrt(np.sum((f**2) * P1) / np.sum(P1))
    
    # NƒÉng l∆∞·ª£ng d·∫£i t·∫ßn th·∫•p (D∆∞·ªõi 50Hz)
    low_freq_indices = f <= 50
    low_freq_energy = np.sum(P1[low_freq_indices]**2)
    
    # T·∫ßn s·ªë c√≥ bi√™n ƒë·ªô c·ª±c ƒë·∫°i (b·ªè qua DC/0Hz)
    peak_freq_index = np.argmax(P1[1:]) + 1
    peak_frequency = f[peak_freq_index]
    
    return [total_energy, center_frequency, rms_frequency, low_freq_energy, peak_frequency]

# --- 3. QUY TR√åNH T·∫†O M√î H√åNH ---
features_list = []
current_idx = 0

# 3.1. Chia c·ª≠a s·ªï v√† Tr√≠ch xu·∫•t
while current_idx + W <= len(raw_data_normal):
    window_data = raw_data_normal[current_idx : current_idx + W]
    feature_vector = extract_frequency_features(window_data, W, FS)
    features_list.append(feature_vector)
    current_idx += S
    
X_normal = np.array(features_list)
print(f"T·ªïng s·ªë m·∫´u ƒë·∫∑c tr∆∞ng Normal ƒë√£ t·∫°o (X_normal): {X_normal.shape[0]}")
# K·∫øt qu·∫£: (1171, 5)

# 3.2. Chu·∫©n h√≥a D·ªØ li·ªáu (R·∫•t quan tr·ªçng cho SVM)
# Ch√∫ng ta FIT (h·ªçc) Scaler tr√™n d·ªØ li·ªáu Normal n√†y
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_normal)
print(f"K√≠ch th∆∞·ªõc d·ªØ li·ªáu sau khi chu·∫©n h√≥a: {X_train_scaled.shape}")

# 3.3. Hu·∫•n luy·ªán One-Class SVM
# nu: T·ª∑ l·ªá d·ªØ li·ªáu Normal b·ªã coi l√† outliers (v√≠ d·ª•: 5%)
nu_value = 0.05 
oc_svm = OneClassSVM(kernel='rbf', gamma='auto', nu=nu_value) 
oc_svm.fit(X_train_scaled)

print("‚úÖ Hu·∫•n luy·ªán m√¥ h√¨nh One-Class SVM cho tr·∫°ng th√°i Normal ƒë√£ ho√†n t·∫•t!")

# 3.4. L∆∞u M√¥ h√¨nh v√† Scaler
# B·∫°n c·∫ßn l∆∞u c·∫£ hai file n√†y ƒë·ªÉ s·ª≠ d·ª•ng trong b∆∞·ªõc ch·∫©n ƒëo√°n Real-time.
joblib.dump(oc_svm, 'ocsvm_normal_model.pkl')
joblib.dump(scaler, 'scaler_normal.pkl')

print("\nüíæ ƒê√£ l∆∞u 2 file ƒë·∫ßu ra quan tr·ªçng: 'ocsvm_normal_model.pkl' v√† 'scaler_normal.pkl'")
